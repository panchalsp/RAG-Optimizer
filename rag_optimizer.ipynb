{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader # PyPDFLoader, UnstructuredPDFLoader,\n",
    "from langchain_experimental.text_splitter import SemanticChunker # RecursiveCharacterTextSplitter # CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = glob.glob('Pdf/*.pdf')\n",
    "all_docs=[]\n",
    "# loader = PyPDFLoader(tmp_filepath) # not properly working. \n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the extracted text\n",
    "def is_valid_doc(doc: Document) -> bool:\n",
    "    text = doc.page_content\n",
    "    clean_text = text.replace('\\x0c', '').strip() # Remove whitespace + form feed (\\x0c) and check length\n",
    "    return len(clean_text) > 30  # or any reasonable threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_path in pdfs[0:100]:\n",
    "    loader = PDFMinerLoader(str(pdf_path))\n",
    "    docs = loader.load()\n",
    "    valid_docs = [doc for doc in docs if is_valid_doc(doc)]\n",
    "    all_docs.extend(valid_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/2919681899.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  HF_chunking = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", cache_folder = '.')\n",
      "/opt/anaconda3/envs/rag/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Splitting the text into chunks\n",
    "\n",
    "# Sematic splitting with openai or huggingFace\n",
    "# openai_chunking = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "HF_chunking = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", cache_folder = '.')\n",
    "chunker = SemanticChunker(\n",
    "    embeddings=HF_chunking,\n",
    "    breakpoint_threshold_type=\"percentile\",       # or \"standard\"\n",
    "    breakpoint_threshold_amount=0.9,           # more strict\n",
    "    min_chunk_size=800,                        # at least 300 token\n",
    ")\n",
    "chunks = chunker.split_documents(all_docs)\n",
    "\n",
    "# CharacterTextSplitter and RecursiveCharacterTextSplitter both works good with seperator '\\n\\n'  \n",
    "# splitter = RecursiveCharacterTextSplitter(separators=\"\\n\\n\", chunk_size=1000, chunk_overlap=200)\n",
    "# chunks = splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/917588646.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
     ]
    }
   ],
   "source": [
    "# Embedding models\n",
    "# HF_MiniLM = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder='.')\n",
    "# HF_MPNet = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", cache_folder='.')\n",
    "# HF_BGE = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", cache_folder='.')\n",
    "openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/3700937651.py:7: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma_db.persist()\n",
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/3700937651.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# Vector stores\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=openai_embedding,\n",
    "    persist_directory=\"./chroma_store\"\n",
    ")\n",
    "chroma_db.persist()\n",
    "\n",
    "faiss_index = FAISS.from_documents(chunks, openai_embedding)\n",
    "faiss_index.save_local(\"faiss_index\")\n",
    "# faiss_index = FAISS.load_local(\"faiss_index\", embeddings=openai_embedding) # Load later\n",
    "\n",
    "qdrant = QdrantClient()  \n",
    "collection_name = \"semantic_chunks\"\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=openai_embedding.embed_query(\"test\").__len__(), distance=Distance.COSINE),\n",
    ") # Create collection\n",
    "qdrant_store = QdrantVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=openai_embedding,\n",
    "    collection_name=collection_name,\n",
    ") # Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_retriever = chroma_db.as_retriever(search_kwargs={\"k\": 5}) # Maximum Marginal Relevance - \n",
    "faiss_retriever = faiss_index.as_retriever(search_kwargs={\"k\": 5})\n",
    "qdrant_retriever = qdrant_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/2224486900.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n"
     ]
    }
   ],
   "source": [
    "# define LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "# RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=qdrant_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/3687721683.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'On which date was this session arranged?',\n",
       " 'result': 'The session was arranged for November 16th at 11:00 AM in 2318 Rayburn House Office Building.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"On which date was this session arranged?\"\n",
    "result = rag_chain(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/9wrs938x0hl8x6c7hm6bwxkh0000gn/T/ipykernel_23999/1367639729.py:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vector_store                                     query  latency_s  \\\n",
      "0       chroma    What is the problem in North Carolina?   1.086196   \n",
      "1       chroma  On which date was this session arranged?   0.323732   \n",
      "2        faiss    What is the problem in North Carolina?   2.381728   \n",
      "3        faiss  On which date was this session arranged?   0.849033   \n",
      "4       qdrant    What is the problem in North Carolina?   0.837467   \n",
      "5       qdrant  On which date was this session arranged?   0.585165   \n",
      "\n",
      "   num_results  \n",
      "0            5  \n",
      "1            5  \n",
      "2            5  \n",
      "3            5  \n",
      "4            5  \n",
      "5            5  \n",
      "\n",
      "Summary:\n",
      "                    min      mean       max       std\n",
      "vector_store                                        \n",
      "chroma        0.323732  0.704964  1.086196  0.539144\n",
      "faiss         0.849033  1.615381  2.381728  1.083779\n",
      "qdrant        0.585165  0.711316  0.837467  0.178404\n"
     ]
    }
   ],
   "source": [
    "# checking the latency\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "retrievers = {\n",
    "    \"chroma\": chroma_retriever,\n",
    "    \"faiss\":  faiss_retriever,\n",
    "    \"qdrant\": qdrant_retriever,\n",
    "}\n",
    "\n",
    "queries = [\n",
    "    \"What is the problem in North Carolina?\",\n",
    "    \"On which date was this session arranged?\",\n",
    "    # …add more queries as needed…\n",
    "]\n",
    "\n",
    "records = []\n",
    "for store_name, retriever in retrievers.items():\n",
    "    for q in queries:\n",
    "        start = time.perf_counter()\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        end   = time.perf_counter()\n",
    "        records.append({\n",
    "            \"vector_store\": store_name,\n",
    "            \"query\":         q,\n",
    "            \"latency_s\":    end - start,\n",
    "            \"num_results\":  len(docs)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df)\n",
    "\n",
    "summary = df.groupby(\"vector_store\")[\"latency_s\"].agg([\"min\",\"mean\",\"max\",\"std\"])\n",
    "print(\"\\nSummary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
